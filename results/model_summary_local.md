# Model Evaluation Summary

| Model (Effort) | ROUGE-L | BLEU | F1 | BERTScore |
|----------------|:-------:|:-----:|:----:|:-----------:|
| **Llama-2-Chat-7b** | 51.86 ± 27.58 | 28.17 ± 27.31 | 58.68 ± 24.22 | 70.22 ± 14.41 |
| **Llama-2-Chat-13b** | 45.18 ± 22.08 | 22.63 ± 23.87 | 53.83 ± 19.48 | 68.16 ± 10.79 |
| **Mistral-Instruct-7b** | 23.58 ± 19.00 | 5.54 ± 11.35 | 31.86 ± 21.67 | 55.98 ± 14.18 |
| **Mixtral-8x7b** | 63.91 ± 26.87 | 46.69 ± 28.42 | 71.21 ± 23.25 | 78.21 ± 14.26 |
| **SOLAR-10.7b** | 63.40 ± 29.03 | 45.25 ± 33.35 | 69.38 ± 25.15 | 78.17 ± 17.39 |
| **Vicuna-13b** | 14.58 ± 14.30 | 3.02 ± 9.61 | 22.72 ± 17.89 | 48.94 ± 13.94 |
| **WizardLM-13b** | 12.96 ± 15.32 | 2.79 ± 10.02 | 19.29 ± 18.51 | 45.70 ± 15.07 |



# Model Evaluation Summary (Only non-empty responses)

| Model (Effort) | ROUGE-L | BLEU | F1 | BERTScore |
|----------------|:-------:|:-----:|:----:|:-----------:|
| **Llama-2-Chat-7b** | 83.91 ± 14.51 | 65.80 ± 17.71 | 86.31 ± 6.96 | 87.15 ± 6.90 |
| **Llama-2-Chat-13b** | 86.73 ± 14.61 | 74.69 ± 16.39 | 86.49 ± 9.58 | 87.51 ± 9.85 |
| **Mistral-Instruct-7b** | 36.05 ± 20.88 | 17.81 ± 15.78 | 44.02 ± 20.96 | 59.48 ± 13.81 |
| **Mixtral-8x7b** | 73.06 ± 18.74 | 57.70 ± 21.56 | 80.07 ± 12.03 | 82.97 ± 9.66 |
| **SOLAR-10.7b** | 83.00 ± 17.36 | 69.90 ± 22.76 | 85.83 ± 14.18 | 89.37 ± 9.39 |
| **Vicuna-13b** | 53.57 ± 33.26 | 46.64 ± 36.75 | 65.11 ± 25.04 | 75.84 ± 16.69 |
| **WizardLM-13b** | 58.72 ± 7.31 | 42.26 ± 9.23 | 71.20 ± 5.44 | 78.29 ± 0.60 |
